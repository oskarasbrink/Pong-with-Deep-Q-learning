{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import DQN\n",
    "import config\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import gym\n",
    "\n",
    "from utils import preprocess\n",
    "from evaluate import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'env_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d17fe6e41024>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'env_config'"
     ]
    }
   ],
   "source": [
    "x = np.random.random((32,4))\n",
    "x = torch.tensor(x)\n",
    "\n",
    "model = DQN()\n",
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%tb` not found (But line magic `%tb` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--env {CartPole-v0}] [--evaluate_freq [EVALUATE_FREQ]]\n",
      "                             [--evaluation_episodes [EVALUATION_EPISODES]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Tobbe\\AppData\\Roaming\\jupyter\\runtime\\kernel-7801b26a-9a0b-4048-adf5-5a837402aeac.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Initialize environment and config.\n",
    "    env = gym.make(args.env)\n",
    "    env_config = ENV_CONFIGS[args.env]\n",
    "\n",
    "    # Initialize deep Q-networks.\n",
    "    dqn = DQN(env_config=env_config).to(device)\n",
    "    # TODO: Create and initialize target Q-network.\n",
    "\n",
    "    # Create replay memory.\n",
    "    memory = ReplayMemory(env_config['memory_size'])\n",
    "\n",
    "    # Initialize optimizer used for training the DQN. We use Adam rather than RMSProp.\n",
    "    optimizer = torch.optim.Adam(dqn.parameters(), lr=env_config['lr'])\n",
    "\n",
    "    # Keep track of best evaluation mean return achieved so far.\n",
    "    best_mean_return = -float(\"Inf\")\n",
    "\n",
    "    for episode in range(env_config['n_episodes']):\n",
    "        done = False\n",
    "\n",
    "        obs = preprocess(env.reset(), env=args.env).unsqueeze(0)\n",
    "        \n",
    "        while not done:\n",
    "            # TODO: Get action from DQN.\n",
    "            action = None\n",
    "\n",
    "            # Act in the true environment.\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            # Preprocess incoming observation.\n",
    "            if not done:\n",
    "                obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
    "            \n",
    "            # TODO: Add the transition to the replay memory. Remember to convert\n",
    "            #       everything to PyTorch tensors!\n",
    "\n",
    "            # TODO: Run DQN.optimize() every env_config[\"train_frequency\"] steps.\n",
    "\n",
    "            # TODO: Update the target network every env_config[\"target_update_frequency\"] steps.\n",
    "\n",
    "        # Evaluate the current agent.\n",
    "        if episode % args.evaluate_freq == 0:\n",
    "            mean_return = evaluate_policy(dqn, env, env_config, args, n_episodes=args.evaluation_episodes)\n",
    "            \n",
    "            print(f'Episode {episode}/{env_config[\"n_episodes\"]}: {mean_return}')\n",
    "\n",
    "            # Save current agent if it has the best performance so far.\n",
    "            if mean_return >= best_mean_return:\n",
    "                best_mean_return = mean_return\n",
    "\n",
    "                print('Best performance so far! Saving model.')\n",
    "                torch.save(dqn, f'models/{args.env}_best.pt')\n",
    "        \n",
    "    # Close environment after training is completed.\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobbe\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#%pip install -U gym>=0.21.0\n",
    "#%pip install -U gym[atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobbe\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v5` with the environment ID `ALE/Pong-v5`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobbe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\win32\\lib\\pywintypes.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp, sys, os\n",
      "C:\\Users\\Tobbe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 55, in <module>\n",
      "    obs, reward, done, info = env.step(action)\n",
      "  File \"C:\\Users\\Tobbe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\wrappers\\time_limit.py\", line 17, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"C:\\Users\\Tobbe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py\", line 13, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"C:\\Users\\Tobbe\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\", line 118, in step\n",
      "    assert self.action_space.contains(action), err_msg\n",
      "AssertionError: None (<class 'NoneType'>) invalid\n"
     ]
    }
   ],
   "source": [
    "!python train.py --env CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
